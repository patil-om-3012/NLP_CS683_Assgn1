{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rt-polarity.pos', 'r', encoding='latin-1') as f:\n",
    "    pos_data = f.readlines()\n",
    "\n",
    "with open('rt-polarity.neg', 'r', encoding='latin-1') as f:\n",
    "    neg_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = ' '.join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = [preprocess_text(sent) for sent in pos_data]\n",
    "neg_data = [preprocess_text(sent) for sent in neg_data]\n",
    "\n",
    "train_pos, val_pos = pos_data[:4000], pos_data[4000:4500]\n",
    "train_neg, val_neg = neg_data[:4000], neg_data[4000:4500]\n",
    "test_pos, test_neg = pos_data[4500:], neg_data[4500:]\n",
    "\n",
    "X_train = train_pos + train_neg\n",
    "y_train = [1] * len(train_pos) + [0] * len(train_neg)\n",
    "\n",
    "X_val = val_pos + val_neg\n",
    "y_val = [1] * len(val_pos) + [0] * len(val_neg)\n",
    "\n",
    "X_test = test_pos + test_neg\n",
    "y_test = [1] * len(test_pos) + [0] * len(test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.vocab = set() \n",
    "        self.word_counts = {0: defaultdict(int), 1: defaultdict(int)}  \n",
    "        self.class_totals = {0: 0, 1: 0}  \n",
    "        self.priors = {0: 0, 1: 0} \n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        total_examples = len(y_train)\n",
    "        self.priors[1] = sum(y_train) / total_examples\n",
    "        self.priors[0] = 1 - self.priors[1]\n",
    "\n",
    "        for sentence, label in zip(X_train, y_train):\n",
    "            words = sentence.split()\n",
    "            self.class_totals[label] += len(words)\n",
    "            for word in words:\n",
    "                self.vocab.add(word)\n",
    "                self.word_counts[label][word] += 1\n",
    "                \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for sentence in X_test:\n",
    "            words = sentence.split()\n",
    "            log_prob_pos = np.log(self.priors[1])\n",
    "            log_prob_neg = np.log(self.priors[0])\n",
    "            \n",
    "            for word in words:\n",
    "                log_prob_pos += np.log((self.word_counts[1][word] + 1) / (self.class_totals[1] + len(self.vocab)))\n",
    "                log_prob_neg += np.log((self.word_counts[0][word] + 1) / (self.class_totals[0] + len(self.vocab)))\n",
    "                \n",
    "            if log_prob_pos > log_prob_neg:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "        return predictions\n",
    "    \n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_val = nb_classifier.predict(X_val)\n",
    "y_pred_test = nb_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Performance:\n",
      "TP: 376, TN: 387, FP: 113, FN: 124\n",
      "F1-Score: 0.7604\n",
      "\n",
      "Test Set Performance:\n",
      "TP: 627, TN: 643, FP: 188, FN: 204\n",
      "F1-Score: 0.7618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "TN_val, FP_val, FN_val, TP_val = cm_val.ravel()\n",
    "\n",
    "f1_val = f1_score(y_val, y_pred_val)\n",
    "\n",
    "print(\"Validation Set Performance:\")\n",
    "print(f\"TP: {TP_val}, TN: {TN_val}, FP: {FP_val}, FN: {FN_val}\")\n",
    "print(f\"F1-Score: {f1_val:.4f}\")\n",
    "\n",
    "\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "TN_test, FP_test, FN_test, TP_test = cm_test.ravel()\n",
    "\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"TP: {TP_test}, TN: {TN_test}, FP: {FP_test}, FN: {FN_test}\")\n",
    "print(f\"F1-Score: {f1_test:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
